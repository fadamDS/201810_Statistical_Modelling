{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Methods for Class Imbalance Issues - Predicting Excellent Wines \n",
    "\n",
    "#### Project for the Statistical Modelling Course - Felix Adam\n",
    "\n",
    "When used properly, statistical classification can help to tackle all sorts of problems: identifiyng fraud, rare diseases or excellent products. However, problems can arise, when the class of interest is severily underrepresented in the data.\n",
    "\n",
    "It is easy to imagine, that the number of observations of the class of interest doesn't exceed 5 percent of all observations. Linear classifiers such as logistic regression tend to perform badly in this setting, due to the fact that they 'learn' the features of the majority class. Predicting that an observation belongs to the majority class is a simple task in this case, while the minority class won't be predicted at all. \n",
    "\n",
    "- Example: Rare disease found in 10 / 1000 people\n",
    "- Classifier just assigns healthy to every person\n",
    "- Will have accuracy of 99% -> Great?\n",
    "\n",
    "An example could be that doctors use an algorithm to classify patients as healthy or not healthy, with respect to a rare disease. The algorithm is trained on a large sample, with potentially thousands of observations. However, if the sample only contains a small amount of 'interesting' cases, the algorithm will learn the healthy cases. When used in action, this will lead to devistating results, given that all sick patients are potentially classified as healthy. \n",
    "\n",
    "Ironically, predicting rare events is one of the most important and interesting tasks in classification. Luckily, there are some techniques for dealing with class imbalances. This notebook is centered around the question of how to deal with such imbalances, following a real data set. \n",
    "I will first discuss the issues that arise when not accounting for class imbalances. Then I'll present undersampling of the majority class as simple remedy. I will later extend this method to synthetic oversampling using the Synthetic Minority Oversampling Technique (SMOTE). In the last section I'll discuss further applications, benefits and shortfalls of the presented techniques and other options available.\n",
    "\n",
    "- My findings are\n",
    "\n",
    "0. The data\n",
    "\n",
    "1. Demonstrate the issue using the wine example\n",
    "    \n",
    "    - Logistic regression seemingly performs well when checking accuracy\n",
    "    - Horrible performance in total, only able to classify \"uninteresting classes\"\n",
    "    - Discussion of ROC, AUC and general assessment of classifiers\n",
    "    - Aiming for high AUC!\n",
    "    - -> Good general performance!\n",
    "    - Provost and Fawcett (1997): Convex hull\n",
    "\n",
    "2. Simple resampling: Undersampling of the majority class\n",
    "\n",
    "    - Discuss method\n",
    "    - Methodology, idea behind it -> Theory\n",
    "    - Show performance compared to base case\n",
    "\n",
    "3. Extension: SMOTE\n",
    "\n",
    "    - Theory, methodology\n",
    "    - Discuss\n",
    "    - Implement and show performance\n",
    "\n",
    "4. Discussion: Which method should be used?\n",
    "\n",
    "    - What about just moving the descision boundary? (Also mentioned in the SMOTE paper)\n",
    "    - Benefits/ Shortfalls\n",
    "    \n",
    "\n",
    "#### Notes\n",
    "\n",
    "- Issue of PREDICTION \n",
    "- Aim of this doc is to present sampling methods, not classifiers\n",
    "- No information rate (see book, page 255)\n",
    "- CV done false: Check your CV set up\n",
    "- When are you using sampling techniques? Before or after CV?\n",
    "- CV at all?\n",
    "- What about flipping the situation around, making the majority class the minority class? Would this make any sense?\n",
    "- Check, whether there is actually a clear linear descision boundary, which could be exploited, otherwise use aritifical data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Wine-Quality Data Set\n",
    "To make this issue more tangible, I will use the wine-quality dataset presented by Cortez et. al (2009). The dataset contains information on red and white wines from the Minho region in Portugal. I will focus on predicting the quality of white wine in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath\n",
    "filepath = \"../data/\"\n",
    "\n",
    "# Loading data\n",
    "whitewine = pd.read_csv(filepath+ 'winequality-white.csv', sep= \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "whitewine.shape[0]": "4898",
     "whitewine.shape[1]": "12"
    }
   },
   "source": [
    "The dataset includes {{whitewine.shape[0]}} observations, each with {{whitewine.shape[1]}} features, where one feature is the quality of the wine (see table below). The wines are assesed by three professional wine tasters who give each wine a grade from 0 (bad) to 10 (excellent). The final grade is calculated as the median of the three values (Cortez et al. 2009). While wine tasting has a bad reputation as being unscientific (Hodgson, 2008), I will assume that the scores actually represent a valid rating. At a minimum, the goal is to predict the taste of these three particular wine tasters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.28</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.051</td>\n",
       "      <td>29.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.99792</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.39</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.39</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.045</td>\n",
       "      <td>59.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>0.99720</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.26</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.048</td>\n",
       "      <td>47.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.36</td>\n",
       "      <td>9.1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.034</td>\n",
       "      <td>30.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.99085</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.40</td>\n",
       "      <td>11.9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4329</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.26</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.054</td>\n",
       "      <td>47.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "3743            7.0              0.15         0.28            14.7      0.051   \n",
       "2706            7.5              0.28         0.39            10.2      0.045   \n",
       "4131            7.4              0.31         0.26             8.6      0.048   \n",
       "838             7.3              0.25         0.36             2.1      0.034   \n",
       "4329            6.4              0.23         0.26             8.1      0.054   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "3743                 29.0                 149.0  0.99792  2.96       0.39   \n",
       "2706                 59.0                 209.0  0.99720  3.16       0.63   \n",
       "4131                 47.0                 206.0  0.99640  3.26       0.36   \n",
       "838                  30.0                 177.0  0.99085  3.25       0.40   \n",
       "4329                 47.0                 181.0  0.99540  3.12       0.49   \n",
       "\n",
       "      alcohol  quality  \n",
       "3743      9.0        7  \n",
       "2706      9.6        6  \n",
       "4131      9.1        5  \n",
       "838      11.9        8  \n",
       "4329      9.4        5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitewine.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (8,6)\n",
    "quality_plt = sns.countplot(x= 'quality', data=whitewine)\n",
    "quality_plt.set_xlabel('Quality')\n",
    "quality_plt.set_ylabel('Nr of Wines')\n",
    "quality_plt.set_title('Distribution of Wine Quality');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Predictions\n",
    "My goal is to predict the quality of a wine using its chemical features. To simplify the estimation and emphasize the issue of class imbalances, I will transform the problem to a 'One vs. Rest' setup (Bishop ,2006, p. 183f). Wines with a quality equal or above 8 will be assigned to have high quality and wines below 8 are classified as low quality.\n",
    "\n",
    "As shown above, the quality of most wines lies between 5 and 7. Transforming the quality to binary values, one can see that high quality wines are severily underrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitewine['quality_class'] = [1 if q >=  8 else 0 for q in whitewine.quality]\n",
    "\n",
    "# Calculating base rate\n",
    "base_rate = round(whitewine['quality_class'].value_counts()[1]/whitewine['quality_class'].value_counts()[0] ,4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     " (base_rate * 100)": "3.82",
     "round((1-base_rate) * 100,2)": "96.18",
     "whitewine['quality_class'].value_counts()[0]": "4718",
     "whitewine['quality_class'].value_counts()[1]": "180"
    }
   },
   "source": [
    "After the transformation, there are {{whitewine['quality_class'].value_counts()[0]}} observations of low quality wines and {{whitewine['quality_class'].value_counts()[1]}} obersvations of high quality wines. The base rate in this case is \n",
    "{{ (base_rate \\* 100)}} percent. It is important to notice, that one could achieve a prediction accuracy of {{round((1-base_rate) * 100,2)}} percent by guessing that all wines presented belong to the low quality class.\n",
    "\n",
    "For my subsequent analysis I will use logistic regression to predict wine quality, while applying different sampling methods. However, in reality, dealing with class imbalances is not just about using different sampling techniques. Rather, as discussed by Weiss (2013), one should first focus on the underlying data-issues at hand. Weiss (2013) mentions problem-definition-level issues, data-level issues and algorithm-level issues. In this set-up, it would be sensible to gather more data on wine-quality in general, or explore different algorithms. \n",
    "\n",
    "- Logistic regression and imbalanced data, maybe short discussion here\n",
    "\n",
    "#### Estimation Set-Up\n",
    "\n",
    "- Ask Omiros, but I would generally follow the procedure described here\n",
    "\n",
    "- Make a training and testing data set, stratified\n",
    "- All methods will be applied on the same training data set (under-, oversampling, smote)\n",
    "- Then for each model fitted on the resulting training set, estimate on the testing data set??\n",
    "\n",
    "\n",
    "- Discuss evaluation metrics in the first naive implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036734693877551024"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Making testing and training data sets ##\n",
    "\n",
    "# Extract features and dependent variable\n",
    "y = whitewine['quality_class']\n",
    "x = whitewine[list(whitewine)[:-2]]\n",
    "\n",
    "# Split into testing and training data, stratified!\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state =0 , stratify = y)\n",
    "\n",
    "# Baseline was 3 percent, verify that stratified sampling worked\n",
    "sum(y_test == 1) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Logistic Regression\n",
    "\n",
    "- I will no start with the baseline logistic regression model without any application of sampling methods\n",
    " Maybe drop discussion of logistic regression?\n",
    "\n",
    "The idea behind logistic regression can be motivated from an example. A winery has collected the grapes of the season and the winegrower has given her best to produce an excellent wine. Now the accountant of the winery comes into play and asks how he should price the wine, given it's quality. Preferrably, he would like to have an idea of how sure he can be that a wine is of particularly high quality, so that he doesn't end up with a mountain of overpriced, low quality wine. Logistic regression can be used in this case to obtain the probability of a wine belonging to the high quality group.\n",
    "\n",
    "Using logistic regression, the probability can be modeled as a logistic transformation of a linear combination of the features of the wine. Given the features $\\phi(x)$, the logg-odds $l$ can be modeled as $l  = ln(\\frac{p}{1-p}) = \\phi(X^T) \\beta$  where $\\beta$ is a vector of parameters to estimate. Then, a logistic transformation can be used to achieve $y \\in (0,1)$ \n",
    "\n",
    "$$\\mu(x,\\beta) = \\frac{1}{1+e^{-\\phi(x^T)\\beta}}$$\n",
    "\n",
    "The conditional mean $\\mu$ depends on the features $X$ and $\\beta$ and $y \\sim Bernoulli( \\mu(x,\\beta) ) $. \n",
    "\n",
    "In the following section I will model the quality of wine through logistic regression. While variable selection and feature engineering play a crucial role in predictive modelling, I will just use all available features for this task. I will try to identify the best predictive model using 10 fold cross validation. \n",
    "\n",
    "- I will make use of pythons sklearn library.\n",
    "- See what works best here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, very small penalty, intercept, 10-Fold Cv, no class weights (default)\n",
    "logistic_model = LogisticRegressionCV(Cs =[1e4], class_weight=None, cv = 10)\n",
    "\n",
    "# Extract features and dependent variable\n",
    "y = whitewine['quality_class']\n",
    "x = whitewine[list(whitewine)[:-2]]\n",
    "\n",
    "\n",
    "# Fit the model \n",
    "logistic_model.fit(x_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on testing values\n",
    "y_pred = logistic_model.predict(x_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test,y_pred)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss what accuracy is\n",
    "\n",
    "The accuracy score of {{round( accuracy \\* 100,2)}} percent looks great at first. After all, roughly 96 percent of the wines have been classified correctly. A look at the confusion matrix however, reveals the issue with relying on the accuracy score.\n",
    "\n",
    "#### AUC \n",
    "\n",
    "- AUC is an accepted traditional performance metric for a ROC Curve (SMOTE Paper),Duda, Hart, & Stork, 2001; Bradley, 1997; Lee, 2000)\n",
    "- ROC Convex Hull?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8,6)\n",
    "\n",
    "# Get confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "\n",
    "# Plot as heatmap\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots() \n",
    "tick_marks = np.arange(len(class_names)) \n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), cmap=\"YlGnBu\", annot=True,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix reveals, that the classifier predicted only a single wine to be of high quality, and that was a wrong prediction. Within the testing set, there were 39 high quality wines, which were all predicted to be of low quality. \n",
    "- More infos here, precision and recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC / AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logistic_model.predict_proba(x_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "fig, ax = plt.subplots() \n",
    "ax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Over / Undersampling using balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, very small penalty, intercept, 10-Fold Cv, no class weights (default)\n",
    "logistic_model2 = LogisticRegression(class_weight='balanced')\n",
    "#LogisticRegressionCV(Cs =[1e4], class_weight='balanced', cv = 10)\n",
    "\n",
    "# Split into testing and training data, stratified!\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state =0 )#, stratify = y)\n",
    "\n",
    "# Fit the model \n",
    "logistic_model2.fit(x_train,y_train);\n",
    "\n",
    "# Make predictions on testing values\n",
    "y_pred = logistic_model2.predict(x_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test,y_pred)\n",
    "\n",
    "metrics.confusion_matrix(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logistic_model2.predict_proba(x_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "fig, ax = plt.subplots() \n",
    "ax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn import over_sampling \n",
    "ros = over_sampling.RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(x, y)\n",
    "\n",
    "# Fit on oversampled data set\n",
    "logistic_model.fit(X_resampled,y_resampled)\n",
    "\n",
    "\n",
    "# Make predictions on testing values\n",
    "y_pred = logistic_model.predict(x_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test,y_pred)\n",
    "\n",
    "metrics.confusion_matrix(y_test,y_pred)\n",
    "\n",
    "#accuracy \n",
    "# Same as setting weights in the fit method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logistic_model2.predict_proba(x_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "fig, ax = plt.subplots() \n",
    "ax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Chawla et al (2002)\n",
    "\n",
    "##### Abstract\n",
    "- Approach for dealing with imbalanced datasets\n",
    "- Imbalance: Not equal representation of classification categories\n",
    "- Happens often in the real world\n",
    "- Cost of misclassifying an \"interesting\" example are a lot higer than misclassifying \"normal\" examples\n",
    "- Paper shows a combination of over-sampling the minority class and under-sampling the majority class\n",
    "- This can achive better performance in ROC space than only under-sampling the majority class\n",
    "- Method involves creating synthetic minority class examples\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "- Imbalance of order 100 to 1 can be found in fraud detection \n",
    "- others report imbalances of up to 100.000 to 1 (Provost & Fawcett, 2001)\n",
    "- Performance is often evaluated using predictive accuracy \n",
    "- Not appropriate when data is imbalanced and/or costs of different errors vary \n",
    "- Example: Cancer prediction / mammography \n",
    "- In the case of severe imbalance, a simple strategy of always predicting the majority class would give an almost perfect predicitve accuracy\n",
    "- Simple predictive accuracy is not appropriate in such situations!\n",
    "- AUC is an accepted tradional performance metric \n",
    "-  ROC Convex Hull -> Very interesting, check it out!\n",
    "\n",
    "How has the machine learning community addressed the issue of class imbalance?\n",
    "- Assign distinct costs to training examples\n",
    "- Re-sample the original dataset\n",
    "        - Either over sample the minority class\n",
    "        - And / Or under-sample the majority class\n",
    "    \n",
    "Approach presented in the paper uses under-sampling of the majority class with a special form of over-sampling of the minority class\n",
    "\n",
    "##### Performance Measures\n",
    "- Performance is typically evaluated by a confusion matrix\n",
    "- In the context of balanced datasets and equal error costs, it is reasonable to use error the error rate as a performance metric\n",
    "$$ error = 1- Accuracy$$\n",
    "\n",
    "- In presence of imbalanced datasets with unequal error costs, more appropriate to use the ROC curve\n",
    "- ROC curves can be thought of as representing the family of best decision boundaries for relative costs of TP and FP\n",
    "- X axis: % False positives\n",
    "- Y axis: % True positives\n",
    "- Ideal point on the ROC curve would be (0,100), all positive examples are classified correctly and no negative examples are misclassified as positive\n",
    "- One way a ROC curve can be swept out is by manipulating the balance of training samples for each class in a training set\n",
    "\n",
    "##### Previous Work: Imbalanced datasets \n",
    "- Kubat and Matwin (1997) selectively under-sample the majority class while keeping the original population of the minority class\n",
    "- Minority examples were divided into four categories: Some noise overlapping the positive class descision regon, borderline samples, redundant samples and safe samples\n",
    "- Ling and Li (1998): Combine over-sampling the minority class with under-sampling the majority class\n",
    "- used lift analysis instead of accuracy\n",
    "- lift curve is more similar to an ROC curve, but more tailored to marketing\n",
    "- Solberg and Solberg (1996): Oil-slick classification, balance 50-50, achieved 14% error rate on the interesting samples and 4 % error on the \"negative\" samples\n",
    "- Precision and recall\n",
    "\n",
    "*Literature summary: Under-sampling the majority class enables better classifiers to be built than over-sampling the minority class!*\n",
    "-> Very interesting summary, check?\n",
    "- Combination of the two as done in previous work does not lead to classifiers that outperform those built with undersampling of majority\n",
    "- Oversampling of the minority class has been donw by sampling with replacement from the original data, this might be an issue -> SMOTE\n",
    "\n",
    "#### SMOTE\n",
    "\n",
    "- Previous research discussed over-sampling with replacment and showed that it doesn't significantly improve minority class recognition\n",
    "- Authors argue with underlying effect in terms of descision region feature space\n",
    "- As the minority class is over-sampled by increasing amounts, the effect is to identify similar but more specific regions in the feature space as the descision region for the minority class\n",
    "\n",
    "Understanding over- and undersampling in terms of descision regions\n",
    "-> Descision boundary for linear regression?\n",
    "\n",
    "- Propose an over-sampling approach in which the minority class is over-sampled by creating synthetic examples, rather than oversampling with replacement\n",
    "- Inspired by handwriting recognition: Used real pictures and perturbed the data a bit to generate new samples\n",
    "- Generate synthetic examples by operating in \"feature space\" not \"data space\"\n",
    "- Taking each minority class sample, introducing synthetic examples along the line segments, joining any / all of the k-miniority class nearest neighbors\n",
    "- Samples are generated by taking the difference between the feature vector (sample) under consideration and it's nearest neighbor.\n",
    "- Multiply this difference by a random number between 0 and 1, add it to the feature vector under consideration\n",
    "- Causes the selection of a random point along the line segment between two specific features\n",
    "- Forces the decision region of the minority class to become more general\n",
    "- The amount of over-sampling is a parameter of the system, series of ROC curves can be generated for different populations and ROC analysis performed\n",
    "\n",
    "- Synthetic samples cause the classifier to create larger and less specific decision regions, rather than smaller and more specific regions\n",
    "- Better for decision trees?\n",
    "\n",
    "##### Under-sampling and SMOTE Combination\n",
    "- Majority class under-sampled, by randomly removing samples until the minority class becomes some specififed percentage of the majority class\n",
    "- Forces the learner to experience varying degrees of under-sampling\n",
    "- By applying a combination of under-sampling and over-sampling, the inital bias of the learner is reversed in favor of the minority class\n",
    "- This is a crucial but somewhat overlooked part of the paper! \n",
    "- The best performance is reached when combining SMOTE and under-sampling of the majority class\n",
    "\n",
    "> Combination of SMOTE and under-sampling is the way to go according to the authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_resampled, y_resampled = SMOTE().fit_resample(x, y)\n",
    "# Fit on oversampled data set\n",
    "logistic_model.fit(x_resampled,y_resampled)\n",
    "\n",
    "\n",
    "# Make predictions on testing values\n",
    "y_pred = logistic_model.predict(x_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test,y_pred)\n",
    "\n",
    "metrics.confusion_matrix(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logistic_model.predict_proba(x_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "fig, ax = plt.subplots() \n",
    "ax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Baseline - Doesn't help much in this case!\n",
    "- Shows that this is not only an issue for logistic regression\n",
    "- other methods are equally prone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on training data\n",
    "modelx = XGBClassifier()\n",
    "\n",
    "# Evaluation set\n",
    "eval_set = [(x_test, y_test)]\n",
    "\n",
    "# Fit the model\n",
    "modelx.fit(x_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=False)\n",
    "\n",
    "# make predictions for test data\n",
    "#y_pred = model.predict(X_test)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "metrics.accuracy_score(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "#y_pred = modelx.predict(x_test)\n",
    "y_pred = modelx.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References \n",
    "\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "  Modeling wine preferences by data mining from physicochemical properties.\n",
    "  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "  \n",
    "He, H., & Ma, Y. (Eds.). (2013). Imbalanced learning: foundations, algorithms, and applications. John Wiley & Sons.\n",
    "\n",
    "G. Weiss (2013). Foundations of Imbalanced Learning, in Imbalanced learning: foundations, algorithms, and applications. edited by He, H., & Ma, Y.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
